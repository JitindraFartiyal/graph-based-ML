{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2151b-c8ee-4224-a44d-b7e9c40a0594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d55a75-65df-4761-8bc9-188ffe44112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90130ee7-82de-42b7-921c-8f10cb1c26a1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c209fb7-f1b7-4ad0-b019-243dbd49a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import KarateClub\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018db812-c86b-46d2-beb3-7f5b01642cda",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3ef7c0-7250-4e3d-a2ac-f5c0646cf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KarateClub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8540ee7-4462-4bf8-8be8-0a7678dc0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d52dd-a306-4696-b1d9-dc5c848bb921",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7558e6ad-a66f-4e03-9cb5-ae7320f2feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_features, 32)\n",
    "        self.conv2 = GCNConv(32, num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        prob = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efd854-d77f-4cea-880f-c8bb9e8ce96d",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd3be688-854b-430f-8d0a-f9a4811d1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device used:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3630bd-3106-4cca-8403-bedf0e12b14a",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7700280c-83ce-4278-a143-f671598b4f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = dataset.num_node_features\n",
    "num_classes = dataset.num_classes\n",
    "node_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2454e896-ebd1-48f4-99b1-29b0634637a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(node_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "482dc35d-3860-4e77-b05a-32aed4374aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa6de4-832b-4c52-9a24-49f698210059",
   "metadata": {},
   "source": [
    "# Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfd4724d-e324-4b10-a419-9c8f05e94670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in dataloader:\n",
    "        sample = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(sample)\n",
    "        \n",
    "        loss = F.nll_loss(out[sample.train_mask], sample.y[sample.train_mask])\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd6174-ffd9-46cb-8b9b-c88367b028ff",
   "metadata": {},
   "source": [
    "# Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "404d76be-d8df-42c9-9d1a-242273146b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd81bbc9-8490-424d-94d7-de595fb67723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train_loss: 0.04450872167944908\n",
      "Epoch: 1 train_loss: 0.047739945352077484\n",
      "Epoch: 2 train_loss: 0.040236398577690125\n",
      "Epoch: 3 train_loss: 0.08639806509017944\n",
      "Epoch: 4 train_loss: 0.03258740156888962\n",
      "Epoch: 5 train_loss: 0.04422576352953911\n",
      "Epoch: 6 train_loss: 0.0523206926882267\n",
      "Epoch: 7 train_loss: 0.10172517597675323\n",
      "Epoch: 8 train_loss: 0.04142517223954201\n",
      "Epoch: 9 train_loss: 0.052501074969768524\n",
      "Epoch: 10 train_loss: 0.04089485853910446\n",
      "Epoch: 11 train_loss: 0.08233557641506195\n",
      "Epoch: 12 train_loss: 0.038822241127491\n",
      "Epoch: 13 train_loss: 0.0587797537446022\n",
      "Epoch: 14 train_loss: 0.03674997389316559\n",
      "Epoch: 15 train_loss: 0.036178432404994965\n",
      "Epoch: 16 train_loss: 0.11324281990528107\n",
      "Epoch: 17 train_loss: 0.03685916215181351\n",
      "Epoch: 18 train_loss: 0.04848182573914528\n",
      "Epoch: 19 train_loss: 0.05629263073205948\n",
      "Epoch: 20 train_loss: 0.04687010869383812\n",
      "Epoch: 21 train_loss: 0.029830701649188995\n",
      "Epoch: 22 train_loss: 0.04352683946490288\n",
      "Epoch: 23 train_loss: 0.03549201413989067\n",
      "Epoch: 24 train_loss: 0.10777328908443451\n",
      "Epoch: 25 train_loss: 0.084716796875\n",
      "Epoch: 26 train_loss: 0.04077831283211708\n",
      "Epoch: 27 train_loss: 0.046343203634023666\n",
      "Epoch: 28 train_loss: 0.04730275273323059\n",
      "Epoch: 29 train_loss: 0.02903261035680771\n",
      "Epoch: 30 train_loss: 0.03510667383670807\n",
      "Epoch: 31 train_loss: 0.038865480571985245\n",
      "Epoch: 32 train_loss: 0.0376688614487648\n",
      "Epoch: 33 train_loss: 0.0581972673535347\n",
      "Epoch: 34 train_loss: 0.031581856310367584\n",
      "Epoch: 35 train_loss: 0.06124689802527428\n",
      "Epoch: 36 train_loss: 0.0393613800406456\n",
      "Epoch: 37 train_loss: 0.03592197224497795\n",
      "Epoch: 38 train_loss: 0.04577510058879852\n",
      "Epoch: 39 train_loss: 0.035472322255373\n",
      "Epoch: 40 train_loss: 0.05643179640173912\n",
      "Epoch: 41 train_loss: 0.061537690460681915\n",
      "Epoch: 42 train_loss: 0.040396224707365036\n",
      "Epoch: 43 train_loss: 0.10413747280836105\n",
      "Epoch: 44 train_loss: 0.05173809826374054\n",
      "Epoch: 45 train_loss: 0.03808480501174927\n",
      "Epoch: 46 train_loss: 0.029609622433781624\n",
      "Epoch: 47 train_loss: 0.05657973140478134\n",
      "Epoch: 48 train_loss: 0.036268200725317\n",
      "Epoch: 49 train_loss: 0.03843491151928902\n",
      "Epoch: 50 train_loss: 0.03361710533499718\n",
      "Epoch: 51 train_loss: 0.06046765670180321\n",
      "Epoch: 52 train_loss: 0.03441663086414337\n",
      "Epoch: 53 train_loss: 0.02595936879515648\n",
      "Epoch: 54 train_loss: 0.024995902553200722\n",
      "Epoch: 55 train_loss: 0.06840067356824875\n",
      "Epoch: 56 train_loss: 0.049097150564193726\n",
      "Epoch: 57 train_loss: 0.06275725364685059\n",
      "Epoch: 58 train_loss: 0.01467608567327261\n",
      "Epoch: 59 train_loss: 0.026751279830932617\n",
      "Epoch: 60 train_loss: 0.04118206351995468\n",
      "Epoch: 61 train_loss: 0.057865772396326065\n",
      "Epoch: 62 train_loss: 0.08879224210977554\n",
      "Epoch: 63 train_loss: 0.05183449015021324\n",
      "Epoch: 64 train_loss: 0.04527677223086357\n",
      "Epoch: 65 train_loss: 0.04646102711558342\n",
      "Epoch: 66 train_loss: 0.04190656170248985\n",
      "Epoch: 67 train_loss: 0.027216443791985512\n",
      "Epoch: 68 train_loss: 0.05451451614499092\n",
      "Epoch: 69 train_loss: 0.07061386108398438\n",
      "Epoch: 70 train_loss: 0.036761779338121414\n",
      "Epoch: 71 train_loss: 0.03204893320798874\n",
      "Epoch: 72 train_loss: 0.053762637078762054\n",
      "Epoch: 73 train_loss: 0.02754213660955429\n",
      "Epoch: 74 train_loss: 0.03388958424329758\n",
      "Epoch: 75 train_loss: 0.02629912458360195\n",
      "Epoch: 76 train_loss: 0.03461387753486633\n",
      "Epoch: 77 train_loss: 0.05219597741961479\n",
      "Epoch: 78 train_loss: 0.029029645025730133\n",
      "Epoch: 79 train_loss: 0.04621538519859314\n",
      "Epoch: 80 train_loss: 0.04088255763053894\n",
      "Epoch: 81 train_loss: 0.04605991020798683\n",
      "Epoch: 82 train_loss: 0.04006379842758179\n",
      "Epoch: 83 train_loss: 0.02255922369658947\n",
      "Epoch: 84 train_loss: 0.057540412992239\n",
      "Epoch: 85 train_loss: 0.055671557784080505\n",
      "Epoch: 86 train_loss: 0.04678603261709213\n",
      "Epoch: 87 train_loss: 0.03728626295924187\n",
      "Epoch: 88 train_loss: 0.02703719586133957\n",
      "Epoch: 89 train_loss: 0.04708223044872284\n",
      "Epoch: 90 train_loss: 0.023570703342556953\n",
      "Epoch: 91 train_loss: 0.030644096434116364\n",
      "Epoch: 92 train_loss: 0.05121025815606117\n",
      "Epoch: 93 train_loss: 0.06817499548196793\n",
      "Epoch: 94 train_loss: 0.028917748481035233\n",
      "Epoch: 95 train_loss: 0.048670411109924316\n",
      "Epoch: 96 train_loss: 0.024261686950922012\n",
      "Epoch: 97 train_loss: 0.055332720279693604\n",
      "Epoch: 98 train_loss: 0.03904685378074646\n",
      "Epoch: 99 train_loss: 0.04482409358024597\n",
      "Epoch: 100 train_loss: 0.04870804026722908\n",
      "Epoch: 101 train_loss: 0.04977041482925415\n",
      "Epoch: 102 train_loss: 0.05556894466280937\n",
      "Epoch: 103 train_loss: 0.07532129436731339\n",
      "Epoch: 104 train_loss: 0.06101103127002716\n",
      "Epoch: 105 train_loss: 0.024922756478190422\n",
      "Epoch: 106 train_loss: 0.04416607320308685\n",
      "Epoch: 107 train_loss: 0.0329507477581501\n",
      "Epoch: 108 train_loss: 0.027702176943421364\n",
      "Epoch: 109 train_loss: 0.04279141500592232\n",
      "Epoch: 110 train_loss: 0.04946529492735863\n",
      "Epoch: 111 train_loss: 0.036687977612018585\n",
      "Epoch: 112 train_loss: 0.029936695471405983\n",
      "Epoch: 113 train_loss: 0.01443510502576828\n",
      "Epoch: 114 train_loss: 0.033124517649412155\n",
      "Epoch: 115 train_loss: 0.06292465329170227\n",
      "Epoch: 116 train_loss: 0.029336653649806976\n",
      "Epoch: 117 train_loss: 0.023162739351391792\n",
      "Epoch: 118 train_loss: 0.03580237925052643\n",
      "Epoch: 119 train_loss: 0.041144274175167084\n",
      "Epoch: 120 train_loss: 0.041039638221263885\n",
      "Epoch: 121 train_loss: 0.041518162935972214\n",
      "Epoch: 122 train_loss: 0.07032977044582367\n",
      "Epoch: 123 train_loss: 0.025113726034760475\n",
      "Epoch: 124 train_loss: 0.04019502177834511\n",
      "Epoch: 125 train_loss: 0.03294391185045242\n",
      "Epoch: 126 train_loss: 0.05360732227563858\n",
      "Epoch: 127 train_loss: 0.037901703268289566\n",
      "Epoch: 128 train_loss: 0.033470962196588516\n",
      "Epoch: 129 train_loss: 0.10294141620397568\n",
      "Epoch: 130 train_loss: 0.04286136478185654\n",
      "Epoch: 131 train_loss: 0.015685424208641052\n",
      "Epoch: 132 train_loss: 0.011985219083726406\n",
      "Epoch: 133 train_loss: 0.031084787100553513\n",
      "Epoch: 134 train_loss: 0.0473392978310585\n",
      "Epoch: 135 train_loss: 0.01635354571044445\n",
      "Epoch: 136 train_loss: 0.01899547129869461\n",
      "Epoch: 137 train_loss: 0.03386108577251434\n",
      "Epoch: 138 train_loss: 0.03322446346282959\n",
      "Epoch: 139 train_loss: 0.030192876234650612\n",
      "Epoch: 140 train_loss: 0.0380694642663002\n",
      "Epoch: 141 train_loss: 0.04978586733341217\n",
      "Epoch: 142 train_loss: 0.04422547668218613\n",
      "Epoch: 143 train_loss: 0.0362418070435524\n",
      "Epoch: 144 train_loss: 0.028989769518375397\n",
      "Epoch: 145 train_loss: 0.029710153117775917\n",
      "Epoch: 146 train_loss: 0.027501262724399567\n",
      "Epoch: 147 train_loss: 0.017039677128195763\n",
      "Epoch: 148 train_loss: 0.030895959585905075\n",
      "Epoch: 149 train_loss: 0.03436527028679848\n",
      "Epoch: 150 train_loss: 0.021446356549859047\n",
      "Epoch: 151 train_loss: 0.03556167706847191\n",
      "Epoch: 152 train_loss: 0.02690170705318451\n",
      "Epoch: 153 train_loss: 0.04200537130236626\n",
      "Epoch: 154 train_loss: 0.03404926881194115\n",
      "Epoch: 155 train_loss: 0.04001219943165779\n",
      "Epoch: 156 train_loss: 0.039059869945049286\n",
      "Epoch: 157 train_loss: 0.04829339683055878\n",
      "Epoch: 158 train_loss: 0.024737851694226265\n",
      "Epoch: 159 train_loss: 0.04470229893922806\n",
      "Epoch: 160 train_loss: 0.030809765681624413\n",
      "Epoch: 161 train_loss: 0.022235773503780365\n",
      "Epoch: 162 train_loss: 0.03412741422653198\n",
      "Epoch: 163 train_loss: 0.04731111228466034\n",
      "Epoch: 164 train_loss: 0.020685195922851562\n",
      "Epoch: 165 train_loss: 0.025667473673820496\n",
      "Epoch: 166 train_loss: 0.05647194758057594\n",
      "Epoch: 167 train_loss: 0.01920914836227894\n",
      "Epoch: 168 train_loss: 0.03520979359745979\n",
      "Epoch: 169 train_loss: 0.043298717588186264\n",
      "Epoch: 170 train_loss: 0.024383341893553734\n",
      "Epoch: 171 train_loss: 0.0235554501414299\n",
      "Epoch: 172 train_loss: 0.024610163643956184\n",
      "Epoch: 173 train_loss: 0.03510882332921028\n",
      "Epoch: 174 train_loss: 0.025938916951417923\n",
      "Epoch: 175 train_loss: 0.05615858733654022\n",
      "Epoch: 176 train_loss: 0.0537269189953804\n",
      "Epoch: 177 train_loss: 0.053910113871097565\n",
      "Epoch: 178 train_loss: 0.04162590205669403\n",
      "Epoch: 179 train_loss: 0.021776411682367325\n",
      "Epoch: 180 train_loss: 0.027971677482128143\n",
      "Epoch: 181 train_loss: 0.02754254639148712\n",
      "Epoch: 182 train_loss: 0.044995203614234924\n",
      "Epoch: 183 train_loss: 0.021930502727627754\n",
      "Epoch: 184 train_loss: 0.05190862715244293\n",
      "Epoch: 185 train_loss: 0.03283011168241501\n",
      "Epoch: 186 train_loss: 0.035339049994945526\n",
      "Epoch: 187 train_loss: 0.029476556926965714\n",
      "Epoch: 188 train_loss: 0.024975012987852097\n",
      "Epoch: 189 train_loss: 0.023210059851408005\n",
      "Epoch: 190 train_loss: 0.03487227112054825\n",
      "Epoch: 191 train_loss: 0.023112155497074127\n",
      "Epoch: 192 train_loss: 0.03419915959239006\n",
      "Epoch: 193 train_loss: 0.02567816898226738\n",
      "Epoch: 194 train_loss: 0.025806233286857605\n",
      "Epoch: 195 train_loss: 0.023353390395641327\n",
      "Epoch: 196 train_loss: 0.029127024114131927\n",
      "Epoch: 197 train_loss: 0.014643870294094086\n",
      "Epoch: 198 train_loss: 0.041115857660770416\n",
      "Epoch: 199 train_loss: 0.022457899525761604\n",
      "Epoch: 200 train_loss: 0.01928900182247162\n",
      "Epoch: 201 train_loss: 0.016673902049660683\n",
      "Epoch: 202 train_loss: 0.025936255231499672\n",
      "Epoch: 203 train_loss: 0.020183483138680458\n",
      "Epoch: 204 train_loss: 0.06168065220117569\n",
      "Epoch: 205 train_loss: 0.03869423642754555\n",
      "Epoch: 206 train_loss: 0.03929182142019272\n",
      "Epoch: 207 train_loss: 0.047923121601343155\n",
      "Epoch: 208 train_loss: 0.018563097342848778\n",
      "Epoch: 209 train_loss: 0.017375916242599487\n",
      "Epoch: 210 train_loss: 0.046517081558704376\n",
      "Epoch: 211 train_loss: 0.033479709178209305\n",
      "Epoch: 212 train_loss: 0.02221442386507988\n",
      "Epoch: 213 train_loss: 0.052910931408405304\n",
      "Epoch: 214 train_loss: 0.028201857581734657\n",
      "Epoch: 215 train_loss: 0.02209075354039669\n",
      "Epoch: 216 train_loss: 0.09381376206874847\n",
      "Epoch: 217 train_loss: 0.022270385175943375\n",
      "Epoch: 218 train_loss: 0.025417901575565338\n",
      "Epoch: 219 train_loss: 0.02728036232292652\n",
      "Epoch: 220 train_loss: 0.04735872521996498\n",
      "Epoch: 221 train_loss: 0.02404734119772911\n",
      "Epoch: 222 train_loss: 0.022337183356285095\n",
      "Epoch: 223 train_loss: 0.028600366786122322\n",
      "Epoch: 224 train_loss: 0.021439826115965843\n",
      "Epoch: 225 train_loss: 0.018383830785751343\n",
      "Epoch: 226 train_loss: 0.04187179356813431\n",
      "Epoch: 227 train_loss: 0.03046535700559616\n",
      "Epoch: 228 train_loss: 0.039729658514261246\n",
      "Epoch: 229 train_loss: 0.017931049689650536\n",
      "Epoch: 230 train_loss: 0.034898072481155396\n",
      "Epoch: 231 train_loss: 0.0269602220505476\n",
      "Epoch: 232 train_loss: 0.01621362194418907\n",
      "Epoch: 233 train_loss: 0.025671251118183136\n",
      "Epoch: 234 train_loss: 0.01989014632999897\n",
      "Epoch: 235 train_loss: 0.018811466172337532\n",
      "Epoch: 236 train_loss: 0.037970300763845444\n",
      "Epoch: 237 train_loss: 0.04308108612895012\n",
      "Epoch: 238 train_loss: 0.024456821382045746\n",
      "Epoch: 239 train_loss: 0.026889875531196594\n",
      "Epoch: 240 train_loss: 0.023158464580774307\n",
      "Epoch: 241 train_loss: 0.02391602285206318\n",
      "Epoch: 242 train_loss: 0.012834607623517513\n",
      "Epoch: 243 train_loss: 0.03266870975494385\n",
      "Epoch: 244 train_loss: 0.023820243775844574\n",
      "Epoch: 245 train_loss: 0.027429815381765366\n",
      "Epoch: 246 train_loss: 0.05057259649038315\n",
      "Epoch: 247 train_loss: 0.03152367100119591\n",
      "Epoch: 248 train_loss: 0.019538894295692444\n",
      "Epoch: 249 train_loss: 0.02380012720823288\n",
      "Epoch: 250 train_loss: 0.022092105820775032\n",
      "Epoch: 251 train_loss: 0.0379042774438858\n",
      "Epoch: 252 train_loss: 0.012077853083610535\n",
      "Epoch: 253 train_loss: 0.05208252742886543\n",
      "Epoch: 254 train_loss: 0.019314147531986237\n",
      "Epoch: 255 train_loss: 0.02105804532766342\n",
      "Epoch: 256 train_loss: 0.027517111971974373\n",
      "Epoch: 257 train_loss: 0.02346653677523136\n",
      "Epoch: 258 train_loss: 0.04519890248775482\n",
      "Epoch: 259 train_loss: 0.01763235218822956\n",
      "Epoch: 260 train_loss: 0.028617998585104942\n",
      "Epoch: 261 train_loss: 0.03023725003004074\n",
      "Epoch: 262 train_loss: 0.022751331329345703\n",
      "Epoch: 263 train_loss: 0.01591786928474903\n",
      "Epoch: 264 train_loss: 0.06834189593791962\n",
      "Epoch: 265 train_loss: 0.03592751547694206\n",
      "Epoch: 266 train_loss: 0.024701813235878944\n",
      "Epoch: 267 train_loss: 0.02728866972029209\n",
      "Epoch: 268 train_loss: 0.028947915881872177\n",
      "Epoch: 269 train_loss: 0.019703779369592667\n",
      "Epoch: 270 train_loss: 0.02123183384537697\n",
      "Epoch: 271 train_loss: 0.032138265669345856\n",
      "Epoch: 272 train_loss: 0.02221105806529522\n",
      "Epoch: 273 train_loss: 0.08638673275709152\n",
      "Epoch: 274 train_loss: 0.023604251444339752\n",
      "Epoch: 275 train_loss: 0.02854570746421814\n",
      "Epoch: 276 train_loss: 0.02876335196197033\n",
      "Epoch: 277 train_loss: 0.020853187888860703\n",
      "Epoch: 278 train_loss: 0.02591492235660553\n",
      "Epoch: 279 train_loss: 0.03462166339159012\n",
      "Epoch: 280 train_loss: 0.03784588724374771\n",
      "Epoch: 281 train_loss: 0.03576266020536423\n",
      "Epoch: 282 train_loss: 0.017362935468554497\n",
      "Epoch: 283 train_loss: 0.02425522916018963\n",
      "Epoch: 284 train_loss: 0.02234848588705063\n",
      "Epoch: 285 train_loss: 0.025758715346455574\n",
      "Epoch: 286 train_loss: 0.04051283001899719\n",
      "Epoch: 287 train_loss: 0.015822315588593483\n",
      "Epoch: 288 train_loss: 0.03825226053595543\n",
      "Epoch: 289 train_loss: 0.02985738217830658\n",
      "Epoch: 290 train_loss: 0.019432032480835915\n",
      "Epoch: 291 train_loss: 0.016161059960722923\n",
      "Epoch: 292 train_loss: 0.03927735984325409\n",
      "Epoch: 293 train_loss: 0.04098368063569069\n",
      "Epoch: 294 train_loss: 0.03525410592556\n",
      "Epoch: 295 train_loss: 0.018086455762386322\n",
      "Epoch: 296 train_loss: 0.020779889076948166\n",
      "Epoch: 297 train_loss: 0.013236735947430134\n",
      "Epoch: 298 train_loss: 0.02033247798681259\n",
      "Epoch: 299 train_loss: 0.0255772415548563\n",
      "Epoch: 300 train_loss: 0.017103996127843857\n",
      "Epoch: 301 train_loss: 0.02462964877486229\n",
      "Epoch: 302 train_loss: 0.03937641903758049\n",
      "Epoch: 303 train_loss: 0.08428926765918732\n",
      "Epoch: 304 train_loss: 0.02239864692091942\n",
      "Epoch: 305 train_loss: 0.02499346062541008\n",
      "Epoch: 306 train_loss: 0.03678195923566818\n",
      "Epoch: 307 train_loss: 0.026351824402809143\n",
      "Epoch: 308 train_loss: 0.013576787896454334\n",
      "Epoch: 309 train_loss: 0.023864541202783585\n",
      "Epoch: 310 train_loss: 0.0456085205078125\n",
      "Epoch: 311 train_loss: 0.020251478999853134\n",
      "Epoch: 312 train_loss: 0.02821737714111805\n",
      "Epoch: 313 train_loss: 0.02114538848400116\n",
      "Epoch: 314 train_loss: 0.033027324825525284\n",
      "Epoch: 315 train_loss: 0.05082863196730614\n",
      "Epoch: 316 train_loss: 0.04599594324827194\n",
      "Epoch: 317 train_loss: 0.01547478698194027\n",
      "Epoch: 318 train_loss: 0.026123959571123123\n",
      "Epoch: 319 train_loss: 0.036077167838811874\n",
      "Epoch: 320 train_loss: 0.022504927590489388\n",
      "Epoch: 321 train_loss: 0.04211912304162979\n",
      "Epoch: 322 train_loss: 0.015164326876401901\n",
      "Epoch: 323 train_loss: 0.021853312849998474\n",
      "Epoch: 324 train_loss: 0.026606887578964233\n",
      "Epoch: 325 train_loss: 0.03452109172940254\n",
      "Epoch: 326 train_loss: 0.030501877889037132\n",
      "Epoch: 327 train_loss: 0.027918145060539246\n",
      "Epoch: 328 train_loss: 0.04437892884016037\n",
      "Epoch: 329 train_loss: 0.016186634078621864\n",
      "Epoch: 330 train_loss: 0.03875875845551491\n",
      "Epoch: 331 train_loss: 0.036324139684438705\n",
      "Epoch: 332 train_loss: 0.01668521575629711\n",
      "Epoch: 333 train_loss: 0.01981467567384243\n",
      "Epoch: 334 train_loss: 0.03938689082860947\n",
      "Epoch: 335 train_loss: 0.006046080030500889\n",
      "Epoch: 336 train_loss: 0.03842789679765701\n",
      "Epoch: 337 train_loss: 0.09677325934171677\n",
      "Epoch: 338 train_loss: 0.01579827256500721\n",
      "Epoch: 339 train_loss: 0.03357582911849022\n",
      "Epoch: 340 train_loss: 0.018049685284495354\n",
      "Epoch: 341 train_loss: 0.02753901481628418\n",
      "Epoch: 342 train_loss: 0.05418936163187027\n",
      "Epoch: 343 train_loss: 0.019452568143606186\n",
      "Epoch: 344 train_loss: 0.02689174748957157\n",
      "Epoch: 345 train_loss: 0.0254339799284935\n",
      "Epoch: 346 train_loss: 0.022357255220413208\n",
      "Epoch: 347 train_loss: 0.023999502882361412\n",
      "Epoch: 348 train_loss: 0.02189025469124317\n",
      "Epoch: 349 train_loss: 0.021068643778562546\n",
      "Epoch: 350 train_loss: 0.03338756784796715\n",
      "Epoch: 351 train_loss: 0.038903236389160156\n",
      "Epoch: 352 train_loss: 0.013851895928382874\n",
      "Epoch: 353 train_loss: 0.024643419310450554\n",
      "Epoch: 354 train_loss: 0.056207120418548584\n",
      "Epoch: 355 train_loss: 0.02285338006913662\n",
      "Epoch: 356 train_loss: 0.01671135239303112\n",
      "Epoch: 357 train_loss: 0.03032834827899933\n",
      "Epoch: 358 train_loss: 0.031478576362133026\n",
      "Epoch: 359 train_loss: 0.024213697761297226\n",
      "Epoch: 360 train_loss: 0.02062787115573883\n",
      "Epoch: 361 train_loss: 0.024850942194461823\n",
      "Epoch: 362 train_loss: 0.04244893789291382\n",
      "Epoch: 363 train_loss: 0.03332434222102165\n",
      "Epoch: 364 train_loss: 0.017591029405593872\n",
      "Epoch: 365 train_loss: 0.02289596199989319\n",
      "Epoch: 366 train_loss: 0.014366187155246735\n",
      "Epoch: 367 train_loss: 0.02498161792755127\n",
      "Epoch: 368 train_loss: 0.014615243300795555\n",
      "Epoch: 369 train_loss: 0.025485193356871605\n",
      "Epoch: 370 train_loss: 0.02956763468682766\n",
      "Epoch: 371 train_loss: 0.014745418913662434\n",
      "Epoch: 372 train_loss: 0.017455151304602623\n",
      "Epoch: 373 train_loss: 0.03139713406562805\n",
      "Epoch: 374 train_loss: 0.02295711636543274\n",
      "Epoch: 375 train_loss: 0.021300848573446274\n",
      "Epoch: 376 train_loss: 0.01625123992562294\n",
      "Epoch: 377 train_loss: 0.017798490822315216\n",
      "Epoch: 378 train_loss: 0.03264441713690758\n",
      "Epoch: 379 train_loss: 0.018212152644991875\n",
      "Epoch: 380 train_loss: 0.007201142143458128\n",
      "Epoch: 381 train_loss: 0.0651932805776596\n",
      "Epoch: 382 train_loss: 0.06895913183689117\n",
      "Epoch: 383 train_loss: 0.012611004523932934\n",
      "Epoch: 384 train_loss: 0.026207532733678818\n",
      "Epoch: 385 train_loss: 0.04668659344315529\n",
      "Epoch: 386 train_loss: 0.03344879671931267\n",
      "Epoch: 387 train_loss: 0.025848036631941795\n",
      "Epoch: 388 train_loss: 0.041226357221603394\n",
      "Epoch: 389 train_loss: 0.03645198047161102\n",
      "Epoch: 390 train_loss: 0.01558231096714735\n",
      "Epoch: 391 train_loss: 0.012445825152099133\n",
      "Epoch: 392 train_loss: 0.03873112425208092\n",
      "Epoch: 393 train_loss: 0.019442208111286163\n",
      "Epoch: 394 train_loss: 0.022857092320919037\n",
      "Epoch: 395 train_loss: 0.030286137014627457\n",
      "Epoch: 396 train_loss: 0.013918486423790455\n",
      "Epoch: 397 train_loss: 0.024330828338861465\n",
      "Epoch: 398 train_loss: 0.026269953697919846\n",
      "Epoch: 399 train_loss: 0.01965544931590557\n",
      "Epoch: 400 train_loss: 0.022162239998579025\n",
      "Epoch: 401 train_loss: 0.019313134253025055\n",
      "Epoch: 402 train_loss: 0.009973214939236641\n",
      "Epoch: 403 train_loss: 0.026727061718702316\n",
      "Epoch: 404 train_loss: 0.008949533104896545\n",
      "Epoch: 405 train_loss: 0.014155921526253223\n",
      "Epoch: 406 train_loss: 0.01785178855061531\n",
      "Epoch: 407 train_loss: 0.025429636240005493\n",
      "Epoch: 408 train_loss: 0.01795080304145813\n",
      "Epoch: 409 train_loss: 0.009922679513692856\n",
      "Epoch: 410 train_loss: 0.014956450089812279\n",
      "Epoch: 411 train_loss: 0.03031102381646633\n",
      "Epoch: 412 train_loss: 0.023477187380194664\n",
      "Epoch: 413 train_loss: 0.019663523882627487\n",
      "Epoch: 414 train_loss: 0.02680087648332119\n",
      "Epoch: 415 train_loss: 0.036049552261829376\n",
      "Epoch: 416 train_loss: 0.01699179969727993\n",
      "Epoch: 417 train_loss: 0.04637179523706436\n",
      "Epoch: 418 train_loss: 0.02281428687274456\n",
      "Epoch: 419 train_loss: 0.018064387142658234\n",
      "Epoch: 420 train_loss: 0.035885922610759735\n",
      "Epoch: 421 train_loss: 0.02386392652988434\n",
      "Epoch: 422 train_loss: 0.0203810203820467\n",
      "Epoch: 423 train_loss: 0.025522392243146896\n",
      "Epoch: 424 train_loss: 0.017677340656518936\n",
      "Epoch: 425 train_loss: 0.016109684482216835\n",
      "Epoch: 426 train_loss: 0.017332397401332855\n",
      "Epoch: 427 train_loss: 0.02303532510995865\n",
      "Epoch: 428 train_loss: 0.03433315083384514\n",
      "Epoch: 429 train_loss: 0.016414346173405647\n",
      "Epoch: 430 train_loss: 0.06470382958650589\n",
      "Epoch: 431 train_loss: 0.026900675147771835\n",
      "Epoch: 432 train_loss: 0.009522121399641037\n",
      "Epoch: 433 train_loss: 0.028328459709882736\n",
      "Epoch: 434 train_loss: 0.015660202130675316\n",
      "Epoch: 435 train_loss: 0.01800243929028511\n",
      "Epoch: 436 train_loss: 0.019588468596339226\n",
      "Epoch: 437 train_loss: 0.008722503669559956\n",
      "Epoch: 438 train_loss: 0.015853114426136017\n",
      "Epoch: 439 train_loss: 0.03511352837085724\n",
      "Epoch: 440 train_loss: 0.04131834954023361\n",
      "Epoch: 441 train_loss: 0.012270776554942131\n",
      "Epoch: 442 train_loss: 0.021955516189336777\n",
      "Epoch: 443 train_loss: 0.020767822861671448\n",
      "Epoch: 444 train_loss: 0.008237885311245918\n",
      "Epoch: 445 train_loss: 0.01121993362903595\n",
      "Epoch: 446 train_loss: 0.018842171877622604\n",
      "Epoch: 447 train_loss: 0.011515134945511818\n",
      "Epoch: 448 train_loss: 0.02601410076022148\n",
      "Epoch: 449 train_loss: 0.013959400355815887\n",
      "Epoch: 450 train_loss: 0.01141201239079237\n",
      "Epoch: 451 train_loss: 0.011746380478143692\n",
      "Epoch: 452 train_loss: 0.03824007511138916\n",
      "Epoch: 453 train_loss: 0.0123448446393013\n",
      "Epoch: 454 train_loss: 0.02204025536775589\n",
      "Epoch: 455 train_loss: 0.021793007850646973\n",
      "Epoch: 456 train_loss: 0.011294258758425713\n",
      "Epoch: 457 train_loss: 0.059737470000982285\n",
      "Epoch: 458 train_loss: 0.01354140229523182\n",
      "Epoch: 459 train_loss: 0.011613879352807999\n",
      "Epoch: 460 train_loss: 0.018616240471601486\n",
      "Epoch: 461 train_loss: 0.019089587032794952\n",
      "Epoch: 462 train_loss: 0.023638620972633362\n",
      "Epoch: 463 train_loss: 0.00730542466044426\n",
      "Epoch: 464 train_loss: 0.023002875968813896\n",
      "Epoch: 465 train_loss: 0.012095441110432148\n",
      "Epoch: 466 train_loss: 0.013118205592036247\n",
      "Epoch: 467 train_loss: 0.01001036074012518\n",
      "Epoch: 468 train_loss: 0.019744304940104485\n",
      "Epoch: 469 train_loss: 0.01422940380871296\n",
      "Epoch: 470 train_loss: 0.029326312243938446\n",
      "Epoch: 471 train_loss: 0.009395178407430649\n",
      "Epoch: 472 train_loss: 0.013524379581212997\n",
      "Epoch: 473 train_loss: 0.020701594650745392\n",
      "Epoch: 474 train_loss: 0.016092631965875626\n",
      "Epoch: 475 train_loss: 0.010370438918471336\n",
      "Epoch: 476 train_loss: 0.01822080835700035\n",
      "Epoch: 477 train_loss: 0.028930475935339928\n",
      "Epoch: 478 train_loss: 0.009206851944327354\n",
      "Epoch: 479 train_loss: 0.017437079921364784\n",
      "Epoch: 480 train_loss: 0.012355542741715908\n",
      "Epoch: 481 train_loss: 0.03912230208516121\n",
      "Epoch: 482 train_loss: 0.009380081668496132\n",
      "Epoch: 483 train_loss: 0.019605547189712524\n",
      "Epoch: 484 train_loss: 0.010644644498825073\n",
      "Epoch: 485 train_loss: 0.009267943911254406\n",
      "Epoch: 486 train_loss: 0.01700959913432598\n",
      "Epoch: 487 train_loss: 0.022118251770734787\n",
      "Epoch: 488 train_loss: 0.010665077716112137\n",
      "Epoch: 489 train_loss: 0.016935572028160095\n",
      "Epoch: 490 train_loss: 0.008172768168151379\n",
      "Epoch: 491 train_loss: 0.015607808716595173\n",
      "Epoch: 492 train_loss: 0.005680509842932224\n",
      "Epoch: 493 train_loss: 0.013779965229332447\n",
      "Epoch: 494 train_loss: 0.02172073908150196\n",
      "Epoch: 495 train_loss: 0.014442735351622105\n",
      "Epoch: 496 train_loss: 0.012872782535851002\n",
      "Epoch: 497 train_loss: 0.019640618935227394\n",
      "Epoch: 498 train_loss: 0.027489133179187775\n",
      "Epoch: 499 train_loss: 0.01350213959813118\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(dataloader, model, optimizer)\n",
    "    print('Epoch:', epoch, 'train_loss:', train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816195f-d33a-4d88-b974-6cb4dd7771cf",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75924b17-0643-4b4c-bd03-a3234a88a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_data = next(iter(dataloader))\n",
    "pred = model(eval_data).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "98c73a36-be64-4732-a5c5-56555696a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = ~eval_data.train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea10a4ea-bc77-47b9-822f-ee374fdf0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "correct = (pred[test_mask] == eval_data.y[test_mask]).sum()\n",
    "acc = int(correct) / int(test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec31dfd-9986-4fea-8c6e-b10102760e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
